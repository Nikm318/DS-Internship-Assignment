{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxoR7mOfbe3/4LvXle4K0l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2gQ7m66Ce-_",
        "outputId": "98ac07b2-1bdc-4781-dc14-41ea55231ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting camelot-py[cv]\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py[cv])\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pypdf, PyMuPDF, pdfminer.six, pdfplumber, camelot-py\n",
            "Successfully installed PyMuPDF-1.26.4 camelot-py-1.0.9 pdfminer.six-20250506 pdfplumber-0.11.7 pypdf-5.9.0 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber PyMuPDF camelot-py[cv] pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "import camelot\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "pJsdS7VkEqBw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== MAIN EXTRACTION FUNCTION ====\n",
        "def extract_pdf_content(pdf_path, output_dir, json_name=\"output.json\", charts_dir=\"charts\"):\n",
        "    \"\"\"\n",
        "    Extracts structured content (paragraphs, tables, charts) from a PDF\n",
        "    and saves as JSON. Chart images are extracted as PNG files.\n",
        "\n",
        "    pdf_path: path to input PDF\n",
        "    output_dir: path to save results\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    charts_path = os.path.join(output_dir, charts_dir)\n",
        "    if not os.path.exists(charts_path):\n",
        "        os.makedirs(charts_path)\n",
        "\n",
        "    json_path = os.path.join(output_dir, json_name)\n",
        "\n",
        "    document_structure = {\"pages\": []}\n",
        "    pdf = fitz.open(pdf_path)\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as plumber_pdf:\n",
        "        for page_number, page in enumerate(plumber_pdf.pages, start=1):\n",
        "            page_data = {\"page_number\": page_number, \"content\": []}\n",
        "\n",
        "            # --- Extract Text (Paragraphs) ---\n",
        "            text = page.extract_text() or \"\"\n",
        "            if text.strip():\n",
        "                sections = split_into_sections(text)\n",
        "                for sec in sections:\n",
        "                    page_data[\"content\"].append({\n",
        "                        \"type\": \"paragraph\",\n",
        "                        \"section\": sec.get(\"section\"),\n",
        "                        \"sub_section\": sec.get(\"sub_section\"),\n",
        "                        \"text\": sec.get(\"content\")\n",
        "                    })\n",
        "\n",
        "            # --- Extract Tables ---\n",
        "            try:\n",
        "                tables = camelot.read_pdf(pdf_path, pages=str(page_number))\n",
        "                for i, table in enumerate(tables):\n",
        "                    page_data[\"content\"].append({\n",
        "                        \"type\": \"table\",\n",
        "                        \"section\": f\"Table {i+1}\",\n",
        "                        \"description\": None,\n",
        "                        \"table_data\": table.df.values.tolist()\n",
        "                    })\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- Detect Images (Charts) ---\n",
        "            page_fitz = pdf[page_number - 1]\n",
        "            image_list = page_fitz.get_images(full=True)\n",
        "            if image_list:\n",
        "                for i, img in enumerate(image_list, start=1):\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(pdf, xref)\n",
        "                    if pix.n > 4:  # Convert CMYK to RGB\n",
        "                        pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    chart_filename = f\"chart_page{page_number}_{i}.png\"\n",
        "                    chart_path = os.path.join(charts_path, chart_filename)\n",
        "                    pix.save(chart_path)\n",
        "                    page_data[\"content\"].append({\n",
        "                        \"type\": \"chart\",\n",
        "                        \"section\": f\"Chart {i}\",\n",
        "                        \"file\": chart_filename,\n",
        "                        \"description\": \"Extracted image (possible chart/figure)\"\n",
        "                    })\n",
        "\n",
        "            document_structure[\"pages\"].append(page_data)\n",
        "\n",
        "    # --- Save JSON ---\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(document_structure, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\" Extraction complete! JSON saved to {json_path}, charts saved in {charts_path}\")\n",
        "\n",
        "\n",
        "# ==== HELPER FUNCTION ====\n",
        "def split_into_sections(text):\n",
        "    \"\"\"\n",
        "    Splits text into sections and sub-sections using regex rules.\n",
        "    \"\"\"\n",
        "    sections = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_section = None\n",
        "    current_sub_section = None\n",
        "    buffer = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Match section headings (1., 2., 3.)\n",
        "        if re.match(r'^\\d+\\.\\s', line):\n",
        "            if buffer:\n",
        "                sections.append({\n",
        "                    \"section\": current_section,\n",
        "                    \"sub_section\": current_sub_section,\n",
        "                    \"content\": \" \".join(buffer).strip()\n",
        "                })\n",
        "                buffer = []\n",
        "            current_section = line\n",
        "            current_sub_section = None\n",
        "\n",
        "        # Match sub-sections (1.1, 2.3.1, etc.)\n",
        "        elif re.match(r'^\\d+(\\.\\d+)+\\s', line):\n",
        "            if buffer:\n",
        "                sections.append({\n",
        "                    \"section\": current_section,\n",
        "                    \"sub_section\": current_sub_section,\n",
        "                    \"content\": \" \".join(buffer).strip()\n",
        "                })\n",
        "                buffer = []\n",
        "            current_sub_section = line\n",
        "\n",
        "        else:\n",
        "            buffer.append(line)\n",
        "\n",
        "    if buffer:\n",
        "        sections.append({\n",
        "            \"section\": current_section or \"General\",\n",
        "            \"sub_section\": current_sub_section,\n",
        "            \"content\": \" \".join(buffer).strip()\n",
        "        })\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "mclzjySXKG2Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to my PDF\n",
        "pdf_file = \"/content/[Fund Factsheet - May]360ONE-MF-May 2025.pdf\"\n",
        "\n",
        "# Path of my Google Drive where results will be saved\n",
        "output_dir = \"/content/drive/MyDrive/PDF_Extraction\"\n",
        "\n",
        "# Run the extraction\n",
        "extract_pdf_content(\n",
        "    pdf_file,\n",
        "    output_dir,\n",
        "    json_name=\"fund_factsheet.json\",\n",
        "    charts_dir=\"charts\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHoK59SzMDbD",
        "outputId": "d755e5a5-ea5b-4321-872a-7f250b5a90aa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Extraction complete! JSON saved to /content/drive/MyDrive/PDF_Extraction/fund_factsheet.json, charts saved in /content/drive/MyDrive/PDF_Extraction/charts\n"
          ]
        }
      ]
    }
  ]
}